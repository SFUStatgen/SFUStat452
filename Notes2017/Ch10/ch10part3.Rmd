---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Chapter 10, part 3: Hierarchical Clustering'
author: "Brad McNeney"
date: '2017-11-21'
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE,fig.height=3,fig.width=5)
```

## Hierarchical Clustering

* Instead of setting the number of clusters in advance, as in $K$-means/medoids,
we create a tree drawing (dendrogram) that represents a hierarchy of
nested partitions of the objects into clusters.
    + See the example on the next slide.
* We can create the hierarchy in a top-down or bottom-up fashion.
* Bottom-up, or agglomerative clustering is the most common and will be
described.
    + Given a measure of dissimilarity between clusters, we successively
    fuse, or merge clusters, starting with $n$ clusters of size one and ending with
    a single cluster of size $n$.

## Example: Hierarchical Clustering of Irises

* First select a subsample of 5 irises from each species, then remove
the species information.
* We use the function `hclust()` to generate the hierarchical clustering.

\scriptsize

```{r}
data(iris)
set.seed(1)
library(dplyr)
iris <- iris %>% 
  group_by(Species) %>%
  sample_n(size=5) %>%
  ungroup() 
irisX <- iris %>%
  select(-Species) %>% 
  scale()
rownames(irisX) <- paste(rownames(iris),iris$Species,sep="_")
ic <- hclust(dist(irisX))
```


## Plotting the Dendrogram

\scriptsize

```{r}
plot(ic,cex=.5)
```

## Interpretation of the Dendrogram

* The height of a node reflects the dissimilarity of its two 
descendant clusters.
    + Branch lengths are not generally informative.
* The first node on the dendrogram partitions the data in to two clear clusters.
    + Knowing the species we can see this reflects the separation
    between the _setosa_ and other species.
* The second node roughly separates the _versicolor_ and _virginica_ species,
though there is one _versicolor_ in the _virginica_ cluster.
    + Note: The subtrees can be rotated without changing the structure of the
    dendrogram, so we should not interpret the horizontal placement of the leaves
    and/or branches.
    
## Simulated Data Example

* Figure 10.10 from the text:

\vspace*{-.4in}

\begin{center}
\includegraphics[height=3in]{Fig10_10.pdf}
\end{center}

* The height of the node that merges $\{9\}$ with $\{2,8,5,7\}$
reflects the dissimilarity. Branch lengths separating, say, 
9 and 2 are not meaningful.

## Cutting Dendrograms to Obtain Clusters

* Cutting the dendrogram at a given dissimilarity value leads to
clusters. 
    + For example, on the iris dendrogram, cutting at about 4 gives two 
    clusters (_setosa_ _vs_ others) and cutting at about 3 gives three 
    clusters (_setosa_ and roughly _vesicolor_ and _virginica_.)
* The `cutree()` function allows us to cut  at either a height `h` 
or where there are `k` clusters.

\scriptsize

```{r}
cutree(ic,k=3)
table(cutree(ic,k=3))
```


## Hierarchical Clustering Algorithm

1. Begin with $n$ observations and a measure of the $n(n-1)/2$ pairwise 
dissimilarities. Treat each observation as its own cluster.
2. For $i = n, n-1, \ldots, 2$:
    (a) Identify the pair of clusters that are least dissimilar and merge 
    them. The dissimilarity between merged clusters is the height of the new
    node on the dendrogram.
    (b) Compute the pairwise inter-cluster dissimilarities among the $i-1$
    remaining clusters.

* To be determined: How do we measure dissimilarity between objects
and between **clusters**?

## Dissimilarity Between Objects

* Several possibilities: 
    + Euclidean ($\ell_2$) distance, $d(a,b) = \sqrt{\sum_{i=1}^p (a_i-b_i)^2}$. 
    + Squared Euclidean ($\ell_2^2$) distance, $d(a,b) = \sum_{i=1}^p (a_i-b_i)^2$.
    + Manhattan ($\ell_1$) distance, $d(a,b) = \sum_{i=1}^p |a_i-b_i|^2$
    + Maximum ($\ell_{\infty}$) distance, $d(a,b) = \max_i |a_i-b_i|$.
* Euclidean, manhattan and maximum distances are implemented in the 
`dist()` function in R, and squared Euclidean can be computed with
`dist(x,method="euclidean")^2`.
    
## Linkage: Dissimilarity Between Clusters

* The four most common linkages are:
    + Complete: $\max \,\{\,d(a,b):a\in A,\,b\in B\,\}$.
    + Average: ${\frac  {1}{|A||B|}}\sum _{{a\in A}}\sum _{{b\in B}}d(a,b)$.
    + Centroid: $||c_{s}-c_{t}||$ where $c_{s}$ and $c_{t}$ are the centroids of clusters s and t, respectively.
    + Single: $\min \,\{\,d(a,b):a\in A,\,b\in B\,\}$.
* According to the text, average, complete and single linkage are the
most popular among statisticians, and average and complete are preferred
because they produce more balanced dendrograms than single.
* In `hclust()`, complete is the default, and the three others are 
options.

## Example: Clustering of the Simulated Data Example

* Figure 10.11 from the text: The first three steps of 
clustering using Euclidean distance and 
complete linkage.

\vspace*{-.2in}

\begin{center}
\includegraphics[height=2.5in]{Fig10_11.pdf}
\end{center}

* Read top-left, top-right, bottom-left, bottom-right.

## The Choice of Dissimilarity, Linkage and Scaling Affect the Dendrogram

* Each choice will influence the dendrogram. 
* Illustrate sensitivity to linkage and scaling.
  
## Sensitivity to Linkage

* Figure 10.10 from the text:

\vspace*{-.1in}

\begin{center}
\includegraphics[height=2.5in]{Fig10_12.pdf}
\end{center}

## Sensitivity to Scaling

* Whether to scale or not is problem dependent.
* The amount of variation in a variable will determine how much 
it influences the dissimilarities, and therefore the linkages
between clusters.
* Example: In the decathlon data from the week 12 exercises, the
1500m had by far the greatest variance.

```{r,echo=FALSE}
library(FactoMineR) #install.packages("FactoMineR")
data(decathlon)
library(dplyr)
# Data processing strips off row names. Save for later.
rnames <- rownames(decathlon)
# Extract Olympics competition
rnames <- rnames[decathlon$Competition=="OlympicG"]
decathlon <- filter(decathlon,Competition=="OlympicG") 
# Extract data on the 10 events.
decathlon <- decathlon[,1:10] # Extract data on the 10 events.
# In most events, a high score is good, but the opposite is true for running.
# Change the running to scores where high is good by subtracting the times from
# the maximum time
diffmax <- function(x) { max(x)-x }
decathlon <- mutate(decathlon,
                    `100m` = diffmax(`100m`),
                    `400m` = diffmax(`400m`),
                    `110m.hurdle` = diffmax(`110m.hurdle`),
                    `1500m` = diffmax(`1500m`))
rownames(decathlon) <- rnames
```

\scriptsize

```{r}
 round(diag(var(decathlon)),3)
```


## Clustering of the Decathlon Data Without Scaling

* Korkizoglou stands apart, because he beat the rest of the field by 
more than 20 seconds in the 1500m.

\scriptsize

```{r}
plot(hclust(dist(decathlon)),cex=.5)
```

## Clustering of the Decathlon Data With Scaling

\scriptsize

```{r}
plot(hclust(dist(scale(decathlon))),cex=.5)
```



  
## NCI60 data

* We follow the lab on clustering of the NCI60 data set, which contains the 
results of a DNA microarray study of 64 cancer cells.
* Cancer cells are labelled by location of the cancer.
    + However, recent research suggests that classification based on
    location of the cancer may not be as useful as classification based
    on the cancer-causing mutation (e.g., a mutation in a gene responsible
    for DNA repair).

## DNA Microarray Experiments

* See the Wikipedia page on microarrays 
(https://en.wikipedia.org/wiki/DNA_microarray)
for a description.
* Briefly:
    + Genes in a cell are transcribed to produce messenger RNA, which is 
    extracted and copied into DNA. 
    + The DNA is fragmented, flourescently
    labelled, and then exposed to an ordered array of complementary DNA molecules
    called probes that identify specific genes.
    + The array "lights up" where the labelled DNA has bound to the probes.
    The flourescence intensity at each probe is a measure of how much 
    of the corresponding gene was being expressed in the cell.
  
## Microarray Picture

* Example micorarray with about 40,000 probes

\vspace*{-.1in}

\begin{center}
\includegraphics[height=2.5in]{Microarray2.pdf}
\end{center}


Source:  [Wikimedia Commons](href=https://commons.wikimedia.org/wiki/File%3AMicroarray2.gif)


## The NCI Data

* Rows are cancer cells, labelled by the type of cancer, and 
columns are the probes (genes).
* Entries of the data matrix are the flourescence intensities after
quality control has been applied.

\scriptsize

```{r}
library(ISLR)
data(NCI60)
nciX <- NCI60$data
dim(nciX)
nciL <- NCI60$labs
rownames(nciX) <- nciL
nciX[1:5,1:5]
```

## PCA on the NCI60 Data

* One could argue that highly expressed genes **should** drive the PCs,
but we scale.

\scriptsize

```{r}
nciX <- scale(nciX)
pcout <- prcomp(nciX)
summary(pcout)
```

## Scree Plot

* Express variances as percent total

\scriptsize

```{r}
pve <- 100*pcout$sdev^2/sum(pcout$sdev^2)
plot(pve,xlim=c(1,20),type="l")
```

\normalsize

* Possible "elbow" at about 5 PCs

## PC Plots

\scriptsize

```{r}
rcols <- rainbow(length(unique(nciL)))
ccols <- rcols[as.numeric(factor(nciL))]
plot(pcout$x[,1],pcout$x[,2],col=ccols,pch=19,xlab="Z1",ylab="Z2")
```

## Pairwise PC plots

\scriptsize

```{r}
pairs(pcout$x[,1:5],col=ccols,pch=19,cex=.5)
```

## $K$-Means Clustering of NCI60 Data

* Remember that `nciX` has already been scaled.
* We know there are 16 different cancer types, but would not specify this
many clusters in practice. 
    + Try $K=4$.

\scriptsize

```{r}
kout <- kmeans(nciX,centers=4)
table(kout$cluster,nciL)
```

##

\scriptsize

```{r}
plot(pcout$x[,1],pcout$x[,2],col=ccols,pch=kout$cluster)
```

## Clustering on PCs

* Can also use the PCs as the data.

\scriptsize

```{r}
kout2 <- kmeans(pcout$x[,1:5],centers=4)
plot(pcout$x[,1],pcout$x[,2],col=ccols,pch=kout2$cluster)
```

## Hierarchical Clustering of NCI60 Data

* Use Euclidean distance and complete linkage
    + See the text for a comparison of complete, average and single linkage
    
\scriptsize

```{r}
hcout <- hclust(dist(nciX))
plot(hcout,cex=.4)
```

## 

\scriptsize

```{r}
plot(pcout$x[,1],pcout$x[,2],col=ccols,pch=cutree(hcout,k=4))
```

\normalsize

* Less satisfying than the $K$-means clusters?





    