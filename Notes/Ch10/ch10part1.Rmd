---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Chapter 10, part 1: Introduction to Unsupervised Learning'
author: "Brad McNeney"
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE,fig.height=3,fig.width=5)
```

## Supervised _versus_ Unsupervised Learning

* Supervised means that there is an outcome $\mathbf{y}$,
unsupervised means there is not.
* Supervised learning has well-defined goals like prediction.
    + Can check the fitted model by seeing how well it predicts
    test observations.
* Unsupervised learning is more exploratory, without an 
obvious goal.
    + A common theme is trying to identify simple
    structure underlying the feature data.
    + We will discuss dimension reduction by principal 
    components analysis (PCA) and clustering.

## Principal Components Analysis (PCA)

* Goal is low-rank approximation of the 
$X$ data matrix
    + Discussed in Chapter 6 and reviewed below. 

* Think of principal components (PCs) as
new coordinates for the data vectors.
    + The first PC is the direction of greatest variation,
    + The second PC is the direction of second-greatest
    variation, orthogonal to the first,
    + And so on.

## PCs for Advertising Data


* Text Figure 6.14:
The green line is the first PC, the blue 
line the second.

\begin{center}
\includegraphics[height=2.5in]{Fig6_14.pdf}
\end{center}

## PCs as Linear Combinations of $X$'s

* The details of how the 
linear combinations are derived are discussed in the text.
* In the advertising example, the first PC is 
$$Z_1 = 0.838 X_1 + 0.544 X_2$$
where $X_1$ is population centred by its mean
and $X_2$ is advertising expenditure centred by its mean.
* The coefficients of the linear combination,
$\phi_{11}= 0.838$ and $\phi_{12}=0.544$,
are called the first principal component *loadings*.

## Principal Component Scores

* Projecting each point onto the PCs gives the 
PC scores.
    + Projecting a data vector 
    onto a line means finding the point
    on the line closest to the vector.

* Text Figure 6.15: Black x's are the first PC score
for each observation, distance of each purple 
dot from the green line is the second PC score.

\vspace*{-.5in}

\begin{center}
\includegraphics[height=2in]{Fig6_15.pdf}
\end{center}

## High-Dimensional Example: Genes Reflect Geography

* First 2 PCs from 
197,146 genetic markers on 1,387 European individuals
(Novembre _et al._ 2008)

\begin{center}
\includegraphics[height=2.5in]{EuropeanGenStr.pdf}
\end{center}

## US Arrests Data

* Dataset that comes with R. 
* From the help file:
"This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas."

\scriptsize

```{r}
data(USArrests) # help(USArrests)
head(USArrests)
```

##

\scriptsize

```{r}
# defaults in prcomp() are to center, but not scale
pcout <- prcomp(USArrests,scale=TRUE) 
pcout$rotation # loadings
head(pcout$x) # scores
```

## Scree Plot

* A scree plot shows the variance (or proportion 
of total variance) in the 
direction of each PC.
* If the variance drops and then levels out, the
"elbow" where it levels out 
is a reasonable choice for a reduced number of PCs
that captures most of the variation in the $\mathbf{X}$.

\scriptsize

```{r}
screeplot(pcout) # or just plot(pcout)
```


\normalsize

* No obvious elbow.

## Iris Data

\scriptsize

```{r}
data(iris) # help(iris)
head(iris)
pcout.i <- prcomp(iris[,-5],scale=TRUE)
```

##

* For the iris data, two PCs appear to explain most
of the variation.

\scriptsize

```{r}
screeplot(pcout.i)
```


## Interpretation of Loadings

* The first PC is a contrast between sepal width and the 
other variables.
* The second PC is a weighted average of sepal length and width.

\scriptsize

```{r}
pcout.i$rotation
```

## Biplot of First Two PCs

* We can visualize the first two PCs on a scatterplot.
* A biplot shows 
    (i) the PC scores for observational units (see left and
    bottom axes), and 
    (ii) the loadings of the features that define the first
    two PCs (see top and right axes)
    

##

\scriptsize

```{r,fig.height=4,fig.width=7}
biplot(pcout.i,cex=.5,scale=0) #scale=0 avoids scaling of points on plot
```


## Biplot of US Arrests Data


\scriptsize

```{r,fig.height=4,fig.width=7}
biplot(pcout,cex=.5,scale=0) #scale=0 avoids scaling of points on plot
```

##

* Note different appearance from text: PCs are only unique 
up to a sign change


## Multiple Correspondence Analysis (MCA)

* An exploratory analysis methodology for 
multivariate datasets with categorical variables.
* In basic form, it is PCA on dummy variables 
that represent the categorical variables.
* Illustrate with the health utilities index (HUI)
variables from the Canadian Community Health
Survey - Healthy Aging

## HUI Data

* The "NOT STATED" response to questions is missing data.

\scriptsize

```{r}
library(tidyverse)
hui <- read.csv("HUI.csv.gz",na.strings = "NOT STATED")
hui[hui=="NA"] <- NA
hui <- na.omit(hui)
names(hui)
dim(hui)
```

## Summaries

\tiny

```{r}
summary(hui) # WTS_M are sampling weights
```

## `HUIDCOG`

* Cognitive function (our focus) with levels:

\small

1. Able to remember most things, think clearly and solve day to day problems
2. Able to remember most things, but have a little difficulty when trying to think and solve day to day problems
3. Somewhat forgetful, but able to think clearly and solve day to day problems
4. Somewhat forgetful, and have a little difficulty when trying to think or solve day to day problems
5. Very forgetful, and have great difficulty when trying to think or solve day to day problems
6. Unable to remember anything at all, and unable to think or solve day to day problems

##

\scriptsize

```{r}
library(dplyr)
levels(hui$HUIDCOG) <- c(as.character(1:6),"NA")
hui %>% group_by(HUIDCOG) %>% summarize(n = sum(WTS_M))
```

## Pairwise summaries

* Relationship between `HUIDCOG` and others

\scriptsize

```{r}
stab <- hui %>% group_by(DHHGAGE,HUIDCOG) %>% 
  summarize(n = sum(WTS_M)) %>% spread(HUIDCOG,n)
stab[,2:7] <- round(stab[,2:7]/rowSums(stab[,2:7]),3)
stab
```

## Pairwise summaries, cont.

* Relationship between `HUIDCOG` and `HUIDEX`.

\scriptsize

```{r}
stab <- hui %>% group_by(HUIGDEX,HUIDCOG) %>% 
  summarize(n = sum(WTS_M)) %>% spread(HUIDCOG,n)

stab[,2:7] <- round(stab[,2:7]/rowSums(stab[,2:7]),3)
stab
```

\normalsize

* And so on ...

## MCA with dummy variables

* We could expand the categorical variables as dummy variables
and do a biplot of the result, but
it is not straightforward to incorporate the weights

## MCA with `FactoMineR`

* The R package `FactoMineR` includes many useful
functions for multivariate data analysis, 
including MCA.
    * Their `MCA()` function includes a weighting argument.

\scriptsize

```{r,cache=TRUE}
library(FactoMineR)
hHUI <- select(hui,HUIDCOG,HUIGDEX,HUIDEMO,HUIGHER,HUIGMOB,
               HUIGSPE,HUIGVIS)
res.mca <- MCA(hHUI,row.w = hui$WTS_M)
```

[SHOW IN RSTUDIO]

