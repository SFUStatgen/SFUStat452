---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Chapter 10: Introduction to Unsupervised Learning'
author: "Brad McNeney"
date: '2017-11-16'
output: 
  beamer_presentation:
    includes:
      in_header: header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE,fig.height=3,fig.width=5)
```

## Supervised _versus_ Unsupervised Learning

* Supervised means that there is an outcome $\mathbf{y}$,
unsupervised means there is not.
* Supervised learning has well-defined goals like prediction.
    + Can check the fitted model by seeing how well it predicts
    test observations.
* Unsupervised learning is more exploratory, without an 
obvious goal.
    + A common theme is trying to identify simple
    structure underlying the feature data.
    + We will discuss dimension reduction by principal 
    components analysis (PCA) and clustering.

## Principal Components Analysis (PCA)

* Goal is low-rank approximation of the 
$X$ data matrix
    + Discussed in Chapter 6 and reviewed below. 

* Think of principal components (PCs) as
new coordinates for the data vectors.
    + The first PC is the direction of greatest variation,
    + The second PC is the direction of second-greatest
    variation, orthogonal to the first,
    + And so on.

## PCs for Advertising Data


* Text Figure 6.14:
The green line is the first PC, the blue 
line the second.

\begin{center}
\includegraphics[height=2.5in]{Fig6_14.pdf}
\end{center}

## PCs as Linear Combinations of $X$'s

* The details of how the 
linear combinations are derived are discussed in the text.
* In the advertising example, the first PC is 
$$Z_1 = 0.838 X_1 + 0.544 X_2$$
where $X_1$ is population centred by its mean
and $X_2$ is advertising expenditure centred by its mean.
* The coefficients of the linear combination,
$\phi_{11}= 0.838$ and $\phi_{12}=0.544$,
are called the first principal component *loadings*.

## Principal Component Scores

* Projecting each point onto the PCs gives the 
PC scores.
    + Projecting a data vector 
    onto a line means finding the point
    on the line closest to the vector.

* Text Figure 6.15: Black x's are the first PC score
for each observation, distance of each purple 
dot from the green line is the second PC score.

\vspace*{-.5in}

\begin{center}
\includegraphics[height=2in]{Fig6_15.pdf}
\end{center}

## High-Dimensional Example: Genes Reflect Geography

* First 2 PCs from 
197,146 genetic markers on 1,387 European individuals
(Novembre _et al._ 2008)

\begin{center}
\includegraphics[height=2.5in]{EuropeanGenStr.pdf}
\end{center}

## US Arrests Data

* Dataset that comes with R. 
* From the help file:
"This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas."

\scriptsize

```{r}
data(USArrests) # help(USArrests)
head(USArrests)
```

##

\scriptsize

```{r}
pcout <- prcomp(USArrests,scale=TRUE)
pcout$rotation # loadings
head(pcout$x) # scores
```

## Scree Plot

* A scree plot shows the variance (or proportion 
of total variance) in the 
direction of each PC.
* If the variance drops and then levels out, the
"elbow" where it levels out 
is a reasonable choice for a reduced number of PCs
that captures most of the variation in the $\mathbf{X}$.

\scriptsize

```{r}
screeplot(pcout) # or just plot(pcout)
```


\normalsize

* No obvious elbow.

## Iris Data

\scriptsize

```{r}
data(iris) # help(iris)
head(iris)
pcout.i <- prcomp(iris[,-5],scale=TRUE)
```

##

* For the iris data, two PCs appear to explain most
of the variation.

\scriptsize

```{r}
screeplot(pcout.i)
```


## Interpretation of Loadings

* The first PC is a contrast between sepal width and the 
other variables.
* The second PC is a weighted average of sepal length and width.

\scriptsize

```{r}
pcout.i$rotation
```

## Biplot of First Two PCs

* We can visualize the first two PCs on a scatterplot.
* A biplot shows 
    (i) the PC scores for observational units, and 
    (ii) the loadings of the features that define the first
    two PCs
    

##

\scriptsize

```{r,fig.height=4,fig.width=7}
biplot(pcout.i,cex=.5,scale=0) #scale=0 avoids scaling of points on plot
```


## Biplot of US Arrests Data


\scriptsize

```{r,fig.height=4,fig.width=7}
biplot(pcout,cex=.5,scale=0) #scale=0 avoids scaling of points on plot
```

##

* Note different appearance from text: PCs are only unique 
up to a sign change



