---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Chapter 3, Part 1: Simple Linear Regression'
author: "Brad McNeney"
output: 
  beamer_presentation:
    includes:
      in_header: ../header_pagenum.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
library(ggplot2)
```


## Example: Advertising Data

* Sales (in thousands of units), and advertising budgets
in thousands of dollars for TV, radio and newspaper for
200 markets.

\scriptsize

```{r}
uu <- url("http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv")
advert <- read.csv(uu,row.names=1)
head(advert)
```

##

\scriptsize

```{r,fig.height=4,fig.width=6}
ggplot(advert,aes(x=TV,y=sales)) + geom_point() +
  geom_smooth(method="lm")
```

## Simple Linear Regression Model

* Recall our general model from Chapter 2:

$$
Y = f(X) + \epsilon
$$

* Simple linear regression assumes the function $f$ is
linear in a single predictor $X$; 
i.e.,  $f(X) = \beta_0 + \beta_1 X$.
    + $\beta_0$ is the intercept and
    + $\beta_1$ is the slope.

## Fitting the line

* We use the method of least squares to fit the line.

* Goal: Using observed data $(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)$
fit the model
$$\hat{y}_i=\hat{\beta_0} + \hat{\beta_1} x_i$$
where $\hat{y}_i$ is the _predicted_ or _fitted value_ of $Y$ for $X=x_i$.
* Idea: try all possible $\hat{\beta_0}$ and $\hat{\beta_1}$ until you find the line that fits the
data the ``best''; i.e. the $\hat{y}$'s are as 
close to the $y$'s as possible. 
    + What is the criteria for best?

## Residuals

* The vertical distances
$e_i = y_i - \hat{y}_i$ are called 
the residuals
    
\begin{center}
\includegraphics[height=2.0in]{Fig3_1.pdf}
\end{center}

\vspace*{-.4in}

## Least Squares

* Least squares minimizes the sum of the squared residuals,
known as the **residual sum of squares**
$$
{\rm RSS} = \sum_{i=1}^n e_i^2
$$

*  There are many visual demonstrations of the least
squares idea on the internet; e.g., 
\url{http://www.dangoldstein.com/regression.html}
    + Try clicking the $-$ slope, $+$ slope, $-$ intercept, and
    $+$ intercept buttons to minimize the sum of squared distances,
    summarized by the blue square.
    + Then click "Fit and lock" to see the line that minimizes
    the sum of squares.


## Least-Squares Regression

* The line that minimizes RSS has
\begin{eqnarray*}
\hat{\beta}_1 & = & r \frac{s_y}{s_x} \\
\hat{\beta}_0 & = & \overline{y} - \hat{\beta}_1 \overline{x}
\end{eqnarray*}
where 
    + $r$ is the Pearson correlation between the $x$'s
    and $y$'s,
    + $s_y$ is the sample SD of the $y$'s and
    $s_x$ is the sample SD of the $x$'s.
* We will always use R to calculate these estimates.

## Advertising Example

\scriptsize

```{r}
afit <- lm(sales ~ TV,data=advert)
summary(afit)$coefficients
```



## Accuracy of the Coefficient Estimates

* Least squares is a good way to fit a line to a scatterplot,
but if we want to assess the accuracy of the coefficient
estimates we need assumptions about the distribution 
of errors.
* Recall the errors $\epsilon$ in $Y = f(X) + \epsilon$.
* Errors are assumed to be normally distributed with mean zero
and SD $\sigma$.
    + The $\epsilon$ are the irreducible error terms, 
    and $\sigma$ quantifies the irreducible error. 
* The SD of the error terms is assumed to be constant for all $x$.

## Model Summary

* We can summarize the model assumptions by saying that:
    + the $(X,Y)$ pairs are independent,
    + conditional on $X=x$, $Y$ has a normal distribution $N(f(x),\sigma)$, with conditional mean
$f(x) = \beta_0 + \beta_1 x$ and conditional standard deviation $\sigma$ being a constant value (i.e. same for all $x$).


## SD and SE of Coefficient Estimators

* Under the model assumptions, one can derive expressions 
for the SD of the sampling distribution of
$\hat{\beta}_0$ and $\hat{\beta}_1$, which 
the text refers to as the standard error (SE). 
    + Side note: What the text calls the SE is what I 
    call the SD, and what the text calls the estimated SE
    is what I call the SE. I'll try to stick to their 
    terminology, but may slip.
* One can derive expressions for the SE and estimated
SE.
    + E.G., equation (3.8) of text, page 66.
    + Both SE and estimated SE denoted $SE(\hat{\beta}_i)$.
    + We will always use the computer to estimate SEs.
    
    
## Simulation Example

* Start R on your computer, choose your own random
seed and run the following sequence of code chunks.

\scriptsize

```{r}
# simulation parameters
n <- 100; beta0 <- 0; beta1 <- 1; sd <- 1; NREPS <- 1000
x <- seq(from=0,to=1,length=n)
#simulation function
simdat <- function() { # R finds sim params from workspace
  f <- beta0 + beta1*x
  y <- f + rnorm(n,mean=0,sd=sd)
  return(list(dat = data.frame(x=x,y=y),coef=coefficients(lm(y~x))))
}
# Set a random seed for replicability
set.seed(1234)
```

##

\scriptsize

```{r,fig.height=3,fig.width=4}
# Do the following a few times
dat <- simdat()$dat
ggplot(dat,aes(x=x,y=y)) + geom_point() + geom_smooth(method="lm") + ylim(-3,3)
```

##

\scriptsize

```{r,cache=TRUE}
simcoef <- function() {
  return(simdat()$coef)
}
simout <- replicate(NREPS,simcoef())
simout <- data.frame(t(simout))
head(simout)
```

##

\scriptsize

```{r,fig.height=3,fig.width=4}
ggplot(simout,aes(x=x)) + geom_density()
```

## Confidence Intervals

* The sampling distribution of the coefficients can be
used to derive the following probability statement:
    + There is a 95% chance that the interval
    $$ 
    (\hat{\beta}_1 - t^* SE(\hat{\beta}_1),
    \hat{\beta}_1 + t^* SE(\hat{\beta}_1))
    $$
    contains the true value of $\beta_1$.
    + $t^*$ is the upper critical value of a 
    $t$ distribution with $n-2$ degrees of freedom (df).
    
    
## Simulation Example

\scriptsize

```{r}
simCI <- function() {
  f <- beta0 + beta1*x
  y <- f + rnorm(n,mean=0,sd=sd)
  ci <- confint(lm(y~x))
  ci["x",]
}
simCI() # Does it contain true value beta1 = 1?
# Exercise: Write code to repeat NREPS times and count 
# how many intervals include beta1.
simout <- replicate(NREPS,simCI())
sum(simout[1,]<= beta1 & simout[2,] >= beta1)
```

## Advertising Example

\scriptsize

```{r}
confint(afit)
```

\normalsize

* We say we are 95% confident that a \$1000 increase
in TV advertising is associated with an
increase in sales of between 42 and 53 units.

## Hypothesis Tests

* The sampling distibution of the coefficients can 
also be used to derive tests of hypotheses about
the parameters.
* Under the null hypothesis that the true 
$\beta_1$ is 0 (no association),
$$
t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}
\sim t_{n-2}
$$

* The usual alternative hypothesis is that $\beta_1 \not= 0$
(association). Then the p-value is the chance of
$T > |t|$, where $T \sim t_{n-2}$.

## Advertising Example

\scriptsize

```{r}
summary(afit)$coefficients
```

\normalsize

* There is very good evidence that increasing TV 
advertising increases sales.


## Accuracy of the Model

* Two common measures of the ability of the model to 
explain variation in $Y$: 
    1. The residual SE (RSE) $\sqrt{RSS/(n-2)}$, which 
    is an estimator of $\sigma$. 
    2. The $R^2 = \frac{TSS-RSS}{TSS}$, where
    $TSS$ is the total sum of squares 
    $$\sum_i (y_i - \bar{y})^2$$
        + $R^2$ is the 
        proportion of variation in $Y$ explained by 
        the regression on $X$.
* The $R^2$ is more commonly used as a goodness-of-fit
measure.

## Advertising Example

\scriptsize

```{r}
summary(afit)
```

\normalsize

* TV advertising explains about 61% of the variation 
in sales.

## Residual Plots

* Residuals
are the primary tool for checking model assumptions.
* For example, a plot of residuals versus fitted values 
can show evidence of
    + departures from linearity -- look for nonlinear trends
    + departures from constant SD -- look for funnel shapes
    + outliers -- unusually large residuals

## Saving the Residuals and Fitted Values

* Use the extractor functions `residuals()` and 
`fitted()`.

\scriptsize

```{r}
advertDiag <- data.frame(advert,residuals=residuals(afit),fitted=fitted(afit))
head(advertDiag)
```


## Plotting Residuals vs. Fitted Values

\scriptsize

```{r, fig.height=2,fig.width=4}
ggplot(advertDiag,aes(x=fitted,y=residuals)) + 
  geom_point() + geom_smooth()
```

\normalsize

* Some evidence of non-linearity on LHS of plot.
* Funnel from left to right.
* Consequences? Tendency to underestimate SE, which 
makes t-statistic too big and p-values too small.
