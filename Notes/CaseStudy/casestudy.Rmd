---
title: 'Statistics 452: Statistical Learning and Prediction'
subtitle: 'Case Study'
author: "Brad McNeney"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
```


## Flights dataset

This is the dataset being analyzed by the Stat 652 class for their
final project.
The data are on flights from three New York City airports 
in 2013, from the `nycflights13` package. 
Data were combined from four datasets from this package:

* `flights`
* `weather`
* `airports`, and
* `planes`

You can read about the variables in each dataset by typing
`help(datasetname)` from the R console. 
Our goal is to predict departure delays (variable `dep_delay` in
minutes).

```{r}
library(tidyverse)
library(nycflights13)
#help(flights)
#help(weather)
#help(airports)
#help(planes)
fltrain <- read_csv("../../Project652/fltrain.csv.gz")
fltrain
```


```{r}
dim(fltrain)
```

There are 43 variables measured on 200,000 flights. 

## Missing data

Handling of missing data is an important topic, but one that we
did not consider in class. Two common ways to deal with missing
data are to (i) remove observations with any missing data (complete-case
analysis) and (ii) impute missing data. Both have their strengths
and limitations. For simplicity we will remove observations. The 
danger is that our inference and/or predictions could be biased, which 
happens when the chance of a missing observation depends on the 
(unobserved) value of the missing data. However, a complete-case
analysis is the most straightforward. 

To ensure we are not discarding too many data points, we limit
the **variables** to those with only a small proportion of missing values.
One rule of thumb is to discard variables with more than 5% missing
values, which is 10,000 for these data. To this end we
count the number of missing values in each variable. 
The character variables have
NA interpreted as a character string. This will be converted to 
the missing code `NA` if we coerce to a factor.

```{r}
fl <- fltrain
for(i in 1:ncol(fl)) {
  if(typeof(fl[[i]]) == "character") {
    fl[[i]] <- factor(fl[[i]])
  }
}

```


Now count the missing values in each variable.

```{r}
num_miss <- function(x) { sum(is.na(x)) }
sapply(fl,num_miss)
```

Some of the variables, particularly those taken from the `planes`
dataset (`year.y` to `engine`), have many missing values. 
 In what follows I'll
discard all of the variables from `planes`, plus `wind_gust` and 
`pressure`.

```{r}
fl <- fl%>% 
  select(-year.y,-type,-manufacturer,-model,-engines,-seats, -speed, -engine,-wind_gust,-pressure)
summary(fl)
```


When we omit rows with any missing values we end up with 184,316 rows out of the origninal 200,000. 

```{r}
fl <- na.omit(fl)
summary(fl)
```

## Summaries of the response variable `dep_delay`

The departure delays variable is highly right-skewed.

```{r}
range(fl$dep_delay)
fivenum(fl$dep_delay)
quantile(fl$dep_delay,probs = c(0.01,0.05,0.1,0.25,.5,.75,.90,.95,.99))
mean(fl$dep_delay >= 60) # about 15,000 or 8% of flights
```

Top 10 delays.

```{r}
fl%>% arrange(desc(dep_delay)) %>% head(10) 
```


Summaries of departure delay by NYC airport:
```{r}
Q3 <- function(x) { quantile(x,probs=.75) }
fl %>% group_by(origin) %>% 
  summarize(n=n(),med_d = median(dep_delay),Q3_d = Q3(dep_delay), max_d = max(dep_delay)) %>% 
  arrange(desc(Q3_d)) %>% head(10) 
```

Summaries of departure delay by airline (carrier).

```{r}
fl %>% group_by(carrier) %>% 
  summarize(n=n(),med_d = median(dep_delay),Q3_d = Q3(dep_delay), max_d = max(dep_delay)) %>% 
  arrange(desc(Q3_d)) %>% head(10) 
fl %>% group_by(origin,carrier) %>% 
  summarize(n=n(),med_d = median(dep_delay),Q3_d = Q3(dep_delay), max_d = max(dep_delay)) %>% 
  arrange(desc(Q3_d)) %>% head(10) 
fl %>% group_by(dest,carrier) %>% 
  summarize(n=n(),med_d = median(dep_delay),Q3_d = Q3(dep_delay), max_d = max(dep_delay)) %>% 
  arrange(desc(Q3_d)) %>% head(10) 
```

Summaries of departure delay by date:

```{r}
fl %>% group_by(month,day) %>% 
  summarize(n=n(),med_d = mean(dep_delay),max_d = max(dep_delay)) %>% 
  arrange(desc(med_d)) %>% head(10) # what happened on march 8?
```


Summaries of departure delay by precipitation:
```{r}
fl %>% mutate(haveprecip = factor(precip>0)) %>% group_by(haveprecip) %>% 
  summarize(n=n(),med_d = median(dep_delay),Q3_d = Q3(dep_delay), max_d = max(dep_delay)) %>% 
  arrange(desc(med_d)) %>% head(10) 
```

## What can we predict?

Extremes seem to be caused by phenomena not in our data, such as snow storms,
mechanical breakdowns (?), etc.

Perhaps we should map these extremes to something less extreme. 
Consider mapping to quantiles of the standard normal (like grading 
departure delays on a "curve"), or mapping to ranks.
We will scale the ranks by $n+1$ to get the empirical 
quantiles, which will be comparable to those in the 
test dataset.

```{r}
#fl <- fl %>% mutate(dep_delay = qqnorm(dep_delay)$x)
den <- nrow(fl)+1
fl <- fl %>% mutate(dep_delay = rank(dep_delay)/den)
 ggplot(fl,aes(x=dep_delay)) + geom_histogram(binwidth=.01)
```

## More data wrangling

* Convert year/month/day to a date object.
* Remove `dep_time`, `arr_time` and `arr_delay`. If we are interested in 
predicting departure delay, these are not know to us.
Also, they may be associated with `dep_delay`, 
but if so the causal effect is likely in reverse.
* Remove `sched_arr_time` (basically departure time + air time),
`tailnum` (4000 planes), `flight` (flight number), `name` (captured by `dest`), 
`air_time` (highly correlated with `distance`), `hour` and `minute` (in `sched_dep_time`),
`time_hour` (same as `sched_dep_time`),
`tz`, `dst`, `tzone` (time zone of destination), 
* Replace numeric `precip` with indicator of precipitation/none.


```{r}
library(lubridate)
fl <- fl %>% 
  mutate(dep_date = make_date(year.x,month,day)) %>% 
  select(-year.x,-month,-day,-dep_time,-arr_time,-arr_delay,
         -sched_arr_time,-tailnum,-flight,-name,-air_time,
         -hour,-minute,-time_hour,-tz,-dst,-tzone) %>%
  mutate(precip = as.numeric(precip>0))
```


## Associations between `dep_delay` and quantitative predictors

Here we look at associations between `dep_delay` rank and other
variables one-at-a-time.
The presentation here is not exhaustive


```{r,cache=TRUE}
ggplot(fl,aes(x=dep_date,y=dep_delay)) + geom_point(alpha=.01) + geom_smooth()
# Definitely non-linear. High in summer, low in fall. Not sure about winter. Looks like
# some sort of event around the end of 2013, but could just be an end effect.
ggplot(fl,aes(x=sched_dep_time,y=dep_delay)) + geom_point(alpha=0.01) + geom_smooth()
# delays increase throughout the day
ggplot(fl,aes(x=distance,y=dep_delay)) + geom_point(alpha=0.01) + geom_smooth()
ggplot(fl,aes(x=log(distance),y=dep_delay)) + geom_point(alpha=0.01) + geom_smooth()
# increases with distance -- use log distance
fl <- mutate(fl,logdistance = log(distance)) %>% select(-distance)
ggplot(fl,aes(x=temp,y=dep_delay)) + geom_point(alpha=0.01) + geom_smooth()
# delays when too hot or too cold
ggplot(fl,aes(x=dewp,y=dep_delay)) + geom_point(alpha=0.01) + geom_smooth()
# similar to temp
# Etc.
# Replace alt with log(alt)
fl <- mutate(fl,logalt = log(alt)) %>% select(-alt)
```

We will likely need to include non-linear terms in the quantitative predictors in our models.

## Split training set in two for tuning

* We have lots of data.
* Methods like cross validation can be used to select tuning parameters, but
approaches like boosting are best with a training/test set.
* Evaluate all learning methods on the test set.

```{r}
set.seed(123)
tr_size <- ceiling(2*nrow(fl)/3)
train <- sample(1:nrow(fl),size=tr_size)
fl_tr <- fl[train,]
fl_te <- fl[-train,]

# baseline to compare learning methods to:
var_dd <- var(fl_te$dep_delay)
var_dd
```


## Learning methods

In the interest of time I'll just consider gam and boosting.

The first fit is a gam with default df for smooths of 
quantitiative variables. As expected, lat, lon and alt of 
the destination contribute very little (not shown) and are removed.

```{r,cache=TRUE}
library(gam)
form <- formula(dep_delay ~ s(dep_date) + s(sched_dep_time) + carrier + origin + dest + s(logdistance) +
                  s(temp) + s(dewp) + s(humid) + s(wind_dir) + s(wind_speed) + precip + s(visib))
gam_fit <- gam(form, data=fl_tr,family=gaussian) 
summary(gam_fit)
plot(gam_fit,se=TRUE)
gam_pred <- predict(gam_fit,newdata=fl_te)
mse_gam <- mean((fl_te$dep_delay-gam_pred)^2)
mse_gam
abs(mse_gam - var_dd)/var_dd
```

The more trees and slower learning, the better a boosted model
will do. I only have time to go as far as 1000 trees and 
shrinkage 0.01.

```{r,cache=TRUE}
library(gbm)
dep_date_numeric <- as.numeric(fl_tr$dep_date)
dep_date_numeric <- dep_date_numeric - mean(dep_date_numeric)
fl_tr_tem <- mutate(fl_tr,dep_date = dep_date_numeric)
gbm_fit <-gbm(dep_delay ~ .,data=fl_tr_tem,distribution="gaussian",
              n.trees = 1000, shrinkage = 0.01)
summary(gbm_fit)
#
dep_date_numeric <- as.numeric(fl_te$dep_date)
dep_date_numeric <- dep_date_numeric - mean(dep_date_numeric)
fl_te_tem <- mutate(fl_te,dep_date = dep_date_numeric)
#
gbm_pred <- predict(gbm_fit,newdata=fl_te_tem,n.trees = 1000)
mse_gbm <- mean((fl_te$dep_delay-gbm_pred)^2)
mse_gbm
abs(mse_gbm - var_dd)/var_dd

```

## Evaluate on Test Data

```{r}
fltest <- read_csv("../../Project652/fltest.csv.gz")
fl <- fltest
for(i in 1:ncol(fl)) {
  if(typeof(fl[[i]]) == "character") {
    fl[[i]] <- factor(fl[[i]])
  }
}
fl <- fl%>% 
  select(-year.y,-type,-manufacturer,-model,-engines,-seats, -speed, -engine,-wind_gust,-pressure)
fl <- na.omit(fl)
fl <- fl %>% 
  mutate(dep_date = make_date(year.x,month,day)) %>% 
  select(-year.x,-month,-day,-dep_time,-arr_time,-arr_delay,
         -sched_arr_time,-tailnum,-flight,-name,-air_time,
         -hour,-minute,-time_hour,-tz,-dst,-tzone) %>%
  mutate(precip = as.numeric(precip>0))
fl <- mutate(fl,logdistance = log(distance)) %>% select(-distance)
fl <- mutate(fl,logalt = log(alt)) %>% select(-alt)
den <- nrow(fl)+1
fl <- fl %>% mutate(dep_delay = rank(dep_delay)/den)
fl_te <- fl
dep_date_numeric <- as.numeric(fl_te$dep_date)
dep_date_numeric <- dep_date_numeric - mean(dep_date_numeric)
fl_te_tem <- mutate(fl_te,dep_date = dep_date_numeric)
#
gbm_pred <- predict(gbm_fit,newdata=fl_te_tem,n.trees = 1000)
mse_gbm <- mean((fl_te$dep_delay-gbm_pred)^2)
mse_gbm
```

